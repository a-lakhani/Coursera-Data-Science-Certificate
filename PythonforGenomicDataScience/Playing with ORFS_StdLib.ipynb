{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "from pprint import pprint\n",
    "\n",
    "path_to_fasta_file = '/home/alakhani/Coursera-Projects/PythonforGenomicDataScience/FASTA_Files/dna.example_copy.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA record descriptions begin with '>', \n",
    "# Simplest way of counting without doing anything else:\n",
    "\n",
    "with open(path_to_fasta_file,'r') as fasta_file:\n",
    "    count = fasta_file.read().count('>')\n",
    "\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary comprehension directly from file looks cool but\n",
    "# does not seem amenable for collecting sequence values in the same iteration.\n",
    "\n",
    "with open(path_to_fasta_file,'r') as fasta_file:\n",
    "    seq_dict = {line.strip('>\\n'): None for line in fasta_file \n",
    "                if line[0] == \">\"}\n",
    "    \n",
    "# print('Length of dictionary made \"the python way\"', len(seq_dict_2),'\\n')\n",
    "# pprint(seq_dict.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8d3a4",
   "metadata": {},
   "source": [
    "## Collecting sequences from a FASTA file into a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dac184",
   "metadata": {},
   "source": [
    "### The more familiar, \"less pythonic\" way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405e184",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fasta_todict(filepath):\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        header = \"\"\n",
    "        seq_accumulator = []\n",
    "        sequences = {}\n",
    "        for line in fasta_file:\n",
    "            # For first line/header.\n",
    "            if line[0] == \">\" and not (header or seq_accumulator):     \n",
    "                header = line.strip('>\\n')\n",
    "            elif line[0] == \">\" and seq_accumulator:\n",
    "                sequences[header] = ''.join(seq_accumulator)\n",
    "                seq_accumulator.clear()\n",
    "                header = line.strip('>\\n')\n",
    "            elif header:\n",
    "                seq_accumulator.append(line.strip())\n",
    "            else:\n",
    "                print(\"Check file.\"\n",
    "                    \"Missing header on first line or two consecutive headers.\")\n",
    "        # Loop ends before the last sequence is paired with header.\n",
    "        sequences[header] = ''.join(seq_accumulator)\n",
    "        return sequences\n",
    "\n",
    "# seqs = fasta_todict(path_to_fasta_file)    \n",
    "# print('Length of sequence dictionary\"', len(sequences))\n",
    "# print(sequences)\n",
    "# partial_list = [item[1] for item in enumerate(seqs.items()) if item[0] < 4]\n",
    "# z = iter(seqs.items())\n",
    "# partial_list = [next(z) for _ in range(4)]\n",
    "# print(list(seqs.items())[:4])\n",
    "# print(partial_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d924b2b",
   "metadata": {},
   "source": [
    "## That was clunky. Is there a cleaner way with itertools?\n",
    "\n",
    "Iterators save memory with 'lazy execution', i.e. they don't start doing the thing until you ask them.\n",
    "This will let us directly feed keys and values into a dictionary without reading into a list first.  \\*\\*\\*\\*\n",
    "\n",
    "We can use a key function to tell `groupby()` to split the file into chunks of consecutive lines that either do or do not start with '>'.\n",
    "`groupby()` returns tuples of `(bool, _grouper)`. `__grouper` is not the sequence chunk, it is itself an iterator that \"knows\" to go through the file and collect lines into chunks, when or if we ask.\n",
    "\n",
    "We need to iterate through (call `__next__` on) the second item in each tuple to tell `__grouper` to actually start iterating through the file and retrieve the next chunk of lines.\n",
    "\n",
    "Calling `next()` retrieves and 'expends' the next value(s) in an iterator (same thing a for loop does), so if `next()` is used in a loop those values or 'positions in the list' will get 'skipped' in the next loop.\n",
    "\n",
    "\\*\\*\\*\\* This was my mistake! Groupby is more memory efficient than reading in the whole file at once and then making a second temporary list. But groupby is still constructing a temporary list of lines for each chunk under the hood, then we're iterating over those temporary lists to join lines. I.e., the same thing we did above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11c73a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joinlines(iterable):\n",
    "    return ''.join(line.strip() for line in iterable)\n",
    "\n",
    "def fasta_todict2(filepath):\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        if fasta_file.readline()[0] != '>':\n",
    "            print(\"File needs to start with header preceded by '>'.\") \n",
    "        else:\n",
    "            fasta_file.seek(0)\n",
    "            chunks = itertools.groupby(fasta_file, key=lambda x: x[0]=='>')\n",
    "            seqdict = {joinlines(chunk[1]).strip('>'): \n",
    "                       joinlines(next(chunks)[1]) for chunk in chunks}\n",
    "            return seqdict\n",
    "\n",
    "# seqdict = fasta_todict2(path_to_fasta_file)\n",
    "\n",
    "# print(list(seqdict.items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to next(), we can zip an iterator with itself.\n",
    "# This will also 'expend' multiple values in one iteration over the zip object.\n",
    "# Note: zip() in Python 3 behaves the same as itertools.izip(). \n",
    "#       zip() in Python 2 returns the entire list at once.  \n",
    "# Also, zipping with the iterator lead to unexpected behavior with the headers \n",
    "# being dropped. Had to turn chunks into a generator to work as expected.\n",
    "#\n",
    "# Zipping is unnecessary for this application since \n",
    "# we're generating tuples just to unpack them.\n",
    "\n",
    "\n",
    "def fasta_todict3(filepath):\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        if fasta_file.readline()[0] != '>':\n",
    "            print(\"File needs to start with header preceded by '>'.\") \n",
    "        else:\n",
    "            fasta_file.seek(0)\n",
    "            chunks = (joinlines(chunk[1]).strip('>') for chunk in \n",
    "                      itertools.groupby(fasta_file, key=lambda x: x[0]=='>'))\n",
    "            seqdict = {chunk1: chunk2 for chunk1, chunk2 in zip(chunks,chunks)}\n",
    "            return seqdict\n",
    "\n",
    "# seqs = fasta_todict3(path_to_fasta_file)\n",
    "\n",
    "# print(list(seqs.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc2cbb",
   "metadata": {},
   "source": [
    "### **Proof that all three methods yield equivalent dictionaries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61159550",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs1 = fasta_todict(path_to_fasta_file)\n",
    "seqs2 = fasta_todict2(path_to_fasta_file)\n",
    "seqs3 = fasta_todict3(path_to_fasta_file)\n",
    "\n",
    "seqs1 == seqs2 == seqs3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e025ca8",
   "metadata": {},
   "source": [
    "### **Did that optimization make any difference for this 59 kb file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "timeit(\"seqs1 = fasta_todict(path_to_fasta_file)\", \n",
    "       \"path_to_fasta_file = '/home/alakhani/Coursera-Projects/PythonforGenomicDataScience/FASTA_Files/dna.example_copy.fasta'\\n\"\n",
    "\"\"\"def fasta_todict(filepath):\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        header = \"\"\n",
    "        seq_accumulator = []\n",
    "        sequences = {}\n",
    "        for line in fasta_file:\n",
    "            # For first line/header.\n",
    "            if line[0] == \">\" and not (header or seq_accumulator):\n",
    "                header = line.strip()\n",
    "            elif line[0] == \">\" and seq_accumulator:\n",
    "                sequences[header] = ''.join(seq_accumulator)\n",
    "                seq_accumulator.clear()\n",
    "                header = line.strip()\n",
    "            elif header:\n",
    "                seq_accumulator.append(line.strip())\n",
    "            else:\n",
    "                print(\"Check file.\")\n",
    "        # Loop ends before the last sequence is paired with header.\n",
    "        sequences[header] = ''.join(seq_accumulator)\n",
    "        return sequences\"\"\",\n",
    "        number=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f53306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "timeit(\"seqs2 = fasta_todict2(path_to_fasta_file)\", \n",
    "       \"path_to_fasta_file = '/home/alakhani/Coursera-Projects/PythonforGenomicDataScience/FASTA_Files/dna.example_copy.fasta'\\n\"\n",
    "\"\"\"def joinlines(iterable):\n",
    "    return ''.join(line.strip() for line in iterable)\n",
    "def fasta_todict2(filepath):\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        if fasta_file.readline()[0] != '>':\n",
    "            print(\"File needs to start with header preceded by '>'.\") \n",
    "        else:\n",
    "            fasta_file.seek(0)\n",
    "            chunks = itertools.groupby(fasta_file, key=lambda x: x[0]=='>')\n",
    "            seqdict = {joinlines(chunk[1]): joinlines(next(chunks)[1]) for \n",
    "                       chunk in chunks}\n",
    "            return seqdict\"\"\",\n",
    "        number=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "timeit(\"seqs3 = fasta_todict3(path_to_fasta_file)\", \n",
    "       \"path_to_fasta_file = '/home/alakhani/Coursera-Projects/PythonforGenomicDataScience/FASTA_Files/dna.example_copy.fasta'\\n\"\n",
    "\"\"\"def joinlines(iterable):\n",
    "    return ''.join(line.strip() for line in iterable)\n",
    "def fasta_todict3(filepath):\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        if fasta_file.readline()[0] != '>':\n",
    "            print(\"File needs to start with header preceded by '>'.\") \n",
    "        else:\n",
    "            fasta_file.seek(0)\n",
    "            chunks = (joinlines(chunk[1]) for chunk in \n",
    "                      itertools.groupby(fasta_file, key=lambda x: x[0]=='>'))\n",
    "            seqdict = {chunk1: chunk2 for chunk1, chunk2 in zip(chunks,chunks)}\n",
    "            return seqdict\"\"\",\n",
    "        number=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b80a27",
   "metadata": {},
   "source": [
    "## Lol\n",
    "Indeed, all that glitters is not gold. The 'dumb way' was faster and will be more readable (to me) in 6 months.\n",
    "\n",
    "It seems the extra overhead from creating extra intermediate objects to perform disjointed nested loops, plus calling a lambda function on each line instead of a simple conditional outweighed the idea that \"it's faster because it's implemented in C\".\n",
    "\n",
    "To be fair, these tools seem more oriented towards managing memory consumption, which is contradicted by copying everything into a dictionary anyway. \n",
    "If we truly needed to avoid copying data unless or until it was needed, it could make more sense to save the `__grouper` objects as dictionary values. However, since the philosophy of Python is to abstract away these types of concerns, maybe it would be unnecessary even then.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932ef7f",
   "metadata": {},
   "source": [
    "## Let's play with dictionaries some more\n",
    "\n",
    "First we'll define `fasta_to_nesteddict` to create a nested dict, so we can associate additional properties with each sequence name.\n",
    "\n",
    "Then we'll pretend we already have a flat dictionary and can't or don't want to create a copycat function. In this case we need to replace dictionary values with a nested dictionary, while retaining the original dictionary value somewhere. \n",
    "\n",
    "#### This highlights one of the advantages of implementing a class rather than a dictionary. What if we want to say, store a property of a property? Class implementation will be explored later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_nesteddict(filepath):\n",
    "    '''Returns {$FASTA_HEADER: {'Sequence': $SEQUENCE}}'''\n",
    "    with open(filepath,'r') as fasta_file:\n",
    "        header = \"\"\n",
    "        seq_accumulator = []\n",
    "        sequences = {}\n",
    "        for line in fasta_file:\n",
    "            # For first line/header.\n",
    "            if line[0] == \">\" and not (header or seq_accumulator):     \n",
    "                header = line.strip('>\\n')\n",
    "            elif line[0] == \">\" and seq_accumulator:\n",
    "                sequences[header] = {'Sequence': ''.join(seq_accumulator)}\n",
    "                seq_accumulator.clear()\n",
    "                header = line.strip('>\\n')\n",
    "            elif header:\n",
    "                seq_accumulator.append(line.strip())\n",
    "            else:\n",
    "                print(\"Check file.\"\n",
    "                    \"Missing header on first line or two consecutive headers.\")\n",
    "        # Loop ends before the last sequence is paired with header.\n",
    "        sequences[header] = {'Sequence': ''.join(seq_accumulator)}\n",
    "        return sequences\n",
    "    \n",
    "# nestedseqdict = fasta_to_nesteddict(path_to_fasta_file)\n",
    "# nestedseqdict_items = iter(nestedseqdict.items())\n",
    "# print([next(nestedseqdict_items) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e13a4",
   "metadata": {},
   "source": [
    "## Adding another level of hierarchy to a flat dictionary \n",
    "\n",
    "In the next function, two parameters are given default values of None, when the intended behavior is to have these parameters default to empty lists. \n",
    "\n",
    "This is due to Python's treatment of mutable default arguments:\n",
    "\n",
    "> <https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument>\n",
    "\n",
    "> <https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments>\n",
    "\n",
    "When functions are defined with default values in Python, objects are initialized with these default values. These default argument *objects*, not the defined default *values*, \"stick with\" the function object throughout the program. \n",
    "\n",
    "If an argument with a mutable default is mutated within a function, calling the function without explicitly declaring this argument will change the argument's \"default value\" for future function calls.\n",
    "This implies a defensive programming practice of explicitly defining mutable arguments if we intend them to be empty.\n",
    "\n",
    "In the function below we do not mutate addkeys or addvals, but it is advisable to avoid mutable defaults as a rule unless this state dependence is specifically intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function requires that addvals support indexing.\n",
    "\n",
    "def nestdict(currentdict, newvalname=\"Old val\", addkeys=None, \n",
    "             addvals: Sequence=None):\n",
    "    '''Replaces dict values with {newvalname: currentdict[key]}. \n",
    "\n",
    "    New key:val pairs can be added to each key's nested dictionary using addkeys and addvals.\n",
    "    Members of addvals can be called on currentdict values.\n",
    "    If addkeys is longer than addvals, unmatched members of addkeys are initialized to None.\n",
    "    Unmatched members of addvals are ignored. \n",
    "    '''\n",
    "    newdict = {}\n",
    "    addkeys = [] if addkeys is None else addkeys\n",
    "    addvals = [] if addvals is None else addvals \n",
    "    for key, val in currentdict.items():\n",
    "        lvl2dict = {}\n",
    "        lvl2dict[newvalname] = val\n",
    "        for index, name in enumerate(addkeys):\n",
    "            try:\n",
    "                x = addvals[index]\n",
    "            except IndexError:\n",
    "                x = None\n",
    "            except:\n",
    "                print(\"addvals in nestdict must be None or a Sequence.\")\n",
    "                raise\n",
    "            if callable(x):\n",
    "                lvl2dict[name] = x(val)\n",
    "            else:\n",
    "                lvl2dict[name] = x\n",
    "        newdict[key] = lvl2dict\n",
    "    if len(addvals) > index + 1:\n",
    "        print(\"More values than keys provided, excess values ignored.\")\n",
    "    return newdict\n",
    "\n",
    "# z = nestdict(fasta_todict(path_to_fasta_file), 'sequence', ['length','ORFs'], [lambda x: len(x), None])\n",
    "# z_items = iter(z.items())\n",
    "# print([next(z_items) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9cde3",
   "metadata": {},
   "source": [
    "**Another approach**\n",
    "\n",
    "`zip_longest()` is cleaner and allows compatibility with iterables that do not support indexing, although the latter seems to have limited benefit. \n",
    "\n",
    "Similar to the previous version, if addvals is longer than addkeys for some reason, key values will default to `None` once addkeys is exhausted.  \n",
    "`zip_longest()` will then generate tuples with the pattern `((None, val_n-2), (None, val_n-1)...(None, val_n))`, each excess value being overwritten until only the last value in addvals is saved.  \n",
    "Managing this will be left to the user. A warning cannot be based on `len` in this case because at no point is there a guarantee that addvals supports `len`.  \n",
    "\n",
    "\\#whenthedocstringisaslongasthefunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nestdict(currentdict, newvalname=\"Old val\", addkeys=None, addvals=None):\n",
    "    '''Replaces dict values with {newvalname: currentdict[key]}. \n",
    "\n",
    "    addkeys and addvals are zipped to generate new key:val pairs for each key's nested dictionary. \n",
    "    Members of addvals can be called on currentdict values.\n",
    "    If addkeys is longer than addvals, unmatched members of addkeys are initialized to None.\n",
    "    If addvals is longer than addkeys, unmatched members of addvals will be lost except for the last element. The last members of the nested dicts will be (None: addvals[n]).\n",
    "    '''\n",
    "    newdict = {}\n",
    "    addkeys = [] if addkeys is None else addkeys\n",
    "    addvals = [] if addvals is None else addvals\n",
    "    for key, val in currentdict.items():\n",
    "        lvl2dict = {}\n",
    "        lvl2dict[newvalname] = val\n",
    "        lvl2dict.update((k, v(val)) if callable(v) else (k, v)\n",
    "                        for k, v in \n",
    "                        itertools.zip_longest(addkeys, addvals))\n",
    "        newdict[key] = lvl2dict\n",
    "    return newdict\n",
    "\n",
    "# z = nestdict(fasta_todict(path_to_fasta_file), 'sequence', ['length','ORFs', 'more', 'keys'], {'dictionary': 'inception', 'this':'is', 'why':'classes', 'are':'probably', 'better': None}.items())\n",
    "# z_items = iter(z.items())\n",
    "# print([next(z_items) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqdict = fasta_todict(path_to_fasta_file)\n",
    "nestedseqdict = fasta_to_nesteddict(path_to_fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e078c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [i for i in seqdict.values()]\n",
    "n = [i['Sequence'] for i in nestedseqdict.values()]\n",
    "s == n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c385",
   "metadata": {},
   "source": [
    "### Well now what if we want to add more values to the nested dictionaries? <sup><sup><sup>or add values to nested nested dictionaries?</sup></sup></sup>\n",
    "\n",
    "\\#needasequenceclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object-oriented, modifies dictionary directly.\n",
    "\n",
    "def update_ndicts_inplace(currentdict, addkeys=None, addvals=None, modkey=''):\n",
    "    '''Members of addvals can optionally be called on nesteddict[modkey].\n",
    "    Uses zip_longest(addkeys, addvals).\n",
    "    '''\n",
    "    addkeys = [] if addkeys is None else addkeys\n",
    "    addvals = [] if addvals is None else addvals\n",
    "    if modkey:\n",
    "        for key, ndict in currentdict.items():\n",
    "            ndict.update((k, v(ndict[modkey])) if callable(v) \n",
    "                        else (k, v) \n",
    "                        for k, v in itertools.zip_longest(addkeys, addvals))\n",
    "    else:\n",
    "        for key, ndict in currentdict.items():\n",
    "            ndict.update((k, v) \n",
    "                        for k, v in itertools.zip_longest(addkeys, addvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure function, returns a new dictionary without modifying the original.\n",
    "from copy import deepcopy\n",
    "\n",
    "def updated_ndict(currentdict, addkeys=None, addvals=None, modkey=''):\n",
    "    '''Members of addvals can be called on nesteddict[modkey].\n",
    "    Uses zip_longest(addkeys, addvals).\n",
    "    '''\n",
    "    addkeys = [] if addkeys is None else addkeys\n",
    "    addvals = [] if addvals is None else addvals\n",
    "    dictcopy = deepcopy(currentdict)\n",
    "    if modkey:\n",
    "        for key, ndict in dictcopy.items():\n",
    "            ndict.update((k, v(ndict[modkey])) if callable(v) \n",
    "                         else (k, v) \n",
    "                         for k, v in itertools.zip_longest(addkeys, addvals))\n",
    "    else:\n",
    "        for key, ndict in dictcopy.items():\n",
    "            ndict.update((k, v) \n",
    "                        for k, v in itertools.zip_longest(addkeys, addvals))\n",
    "    return dictcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ce91d",
   "metadata": {},
   "source": [
    "### Finding ORFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0597f",
   "metadata": {},
   "source": [
    "#### The power of regex\n",
    "\n",
    "ORFs are delimited by a start codon ('ATG' in the case of DNA and 'UAG' in the case of RNA), and an \"in-frame\" stop codon. *Typically* in DNA and RNA, these correspond to 'TAA', 'TAG', and 'TGA', or 'UAA', 'UAG', and 'UGA' respectively. \"In-frame\" means the number of bases between the stop and start codons is a multiple of 3. \n",
    "\n",
    "'ATGATAA' contains two codons (groups of three bases): 'ATG' and 'ATA'. Even though 'TAA' is present in the sequence, it is not recognized as a stop codon because the number of intervening nucleotides is not a multiple of 3. \n",
    "\n",
    "One way to find an ORF is to find the first instance of 'ATG', str.find() instances of 'TAA', 'TAG', and 'TGA' following the start codon. The first tricky bit is that str.find() only searches for a particular sequence and only returns the index of the first location, so we would need separate iterations for each stop codon sequence.\n",
    "\n",
    "We would stop iterating at the first instance where the number of nucleotides between the start of the start codon and the start of the stop codon is a multiple of 3 (n % 3 == 0). \n",
    "\n",
    "Regular expressions can express this problem concisely, in an \"ancient magic\" sort of way. This comes at the price of readability and speed: regex can be more than an order of magnitude slower than built-ins: <https://stackoverflow.com/questions/8958229/is-a-regular-expression-faster-than-using-replace>. However if you want to show how `1337` you are and like Googling how your own code works when you have to look at it six months later, regex is the way to go.\n",
    "\n",
    "Quick breakdown of the expression used below:\n",
    "\n",
    "- Expressions in parenthesis indicate logical separations. `(ATG)` means look for the first instance of the sequence `ATG`. \n",
    "  - Square brackets `[ATG]` would just look for the first instance of 'A' and/or 'T' and/or 'G'. Any of these characters in any order would be sufficient to start the next stage of the search.\n",
    "- `(\\S{3})`: `\\S` (not to be confused with `\\s`!) is a wildcard for any non-whitespace character. The slash needs to be escaped unless using a raw string. \n",
    "  - Braces `{}` are notation for repetition: `{3}` means repeats of 3. Regex also supports looking for a range of repeats, such as `{min,[max]}`.\n",
    "  - The `\\s` could also be replaced by sequences such as `[ATGUCatguc]` or `[A-Z,a-z]`, but DNA and RNA sequences sometimes contain whitespaces between codons.  \n",
    "- `*` means \"zero or more instances\". Combined with the previous expression, `(.{3})*` means \"look for one or more repeats of 3.\n",
    "- `*` uses \"greedy\" evaluation, meaning it will keep going as far as it can and then work backwards to fulfill the rest of the conditions. This yields the maximum number of the preceding expression that still fulfills the other conditions for the expression. So if the expression could be satisfied by a sequence containing 5 or 50 repeats of 3, greedy evaluation would return the sequence containing 50 repeats. This is not what we want! We don't want to read through valid stop codons, we want to stop at stop codons.\n",
    "- `*?` reigns in `*` and switches to lazy evaluation. This means it stops once it finds an expression that satisfies the necessary conditions. This is the intended behavior here. \n",
    " - `??` is super lazy, and looks for the lesser of 0 or 1 of the previous clause.\n",
    "\n",
    "Regex is like a very sharp knife: efficient in the hands of an experienced user, but new users need to take extra care to avoid mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b04122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s1 = 'AYGCTATGCGAGCTASYUYGCTAGUTAATGAYSGC'\n",
    "s2 = 'AYGCTATGCGAGCTAAGATGCGGTAGTAATTTGAYSGC'\n",
    "s3 = 'AYGCTATGCGAGCTGATGCGGTAGTAATTTGAYSGC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = re.search(r'(ATG)(\\S{3})*(TAG|TAA|TGA)',s1)\n",
    "m2a, m2b, m2c = (re.search(r'(ATG)(\\S{3})*(TAG|TAA|TGA)',s2), \n",
    "                 re.search(r'(ATG)(\\S{3})*?(TAG|TAA|TGA)',s2),\n",
    "                 re.search(r'(ATG)(\\S{3})??(TAG|TAA|TGA)',s2))\n",
    "m3a, m3b = (re.search(r'(ATG)(\\S{3})*?(TAG|TAA|TGA)',s3),\n",
    "            re.search(r'(ATG)(\\S{3})??(TAG|TAA|TGA)',s3))\n",
    "print(f'm1: {m1}\\nm2a: {m2a}\\nm2b: {m2b}\\nm2c: {m2c}\\nm3a: {m3a}\\nm3b: {m3b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c04c5d",
   "metadata": {},
   "source": [
    "Now we know how to find the first ORF in a sequence, which I'm defining as the substring containing the first start codon that is followed by an in-frame stop codon. \n",
    "\n",
    "In the example above, m3a is the first ORF even though the sequence of m3b ends first.\n",
    "\n",
    "If we want to find all ORFs, we can slice the string after the start codon of the previous ORF and repeat the search process until re.search() returns `None`. \n",
    "\n",
    "`str[re.Match.start()+3:]` can be used to get the next slice, but the start and end positions of the next ORF will be reported relative to the beginning of the slice, not the original string. The actual index of `re.Match.start()` would need to be saved somewhere if we want to know the position of each ORF in the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83390d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3c = re.search(r'((ATG)(\\S{3})*?(TAG|TAA|TGA))',s3[m3a.start()+3:])\n",
    "print(f'm3c: {m3c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d6887",
   "metadata": {},
   "source": [
    "Things get more tricky if we want to limit our search to a particular reading frame, and thorough testing is needed to make sure we're the behavior is what is intended. The idea is to look for a start codon that is in frame with the beginning of the string, i.e. it's preceded by groups of 3 or its index is divisible by 3. \n",
    "\n",
    "If we just add `(.{3})*?` to the beginning of the expression used above, we just collect all the triplets that precede the first ORF, even if the resulting sequence is out of frame. \n",
    "\n",
    "Adding `^` anchors the expression to the beginning of the string, meaning the triplets have to start from the beginning.\n",
    "\n",
    "Lastly, this strategy returns extraneous codons that precede the ORF. Thankfully re.Match objects save the expressions that were returned by the individual \"logical\" parts of the search pattern. Surrounding the expression that represents the ORF with parentheses `((ATG)(.{3})*?(TAG|TAA|TGA))` will make it easy to extract without the irrelevant portion of the sequence, and parenthesizing the \"triplet tracker\" `((.{3})*)` will help us get an accurate span for the ORF. Otherwise only the penultimate triplet gets saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s4 = 'AYAGCTATGCGAATGGCTGATGCGGTAGTAATCTGAYSGC'\n",
    "\n",
    "m4a = re.search(r'^((\\S{3})*)((ATG)(\\S{3})*?(TAG|TAA|TGA))', s4)\n",
    "m4b = re.search(r'^((\\S{3})*?)((ATG)(\\S{3})*?(TAG|TAA|TGA))', s4)\n",
    "m4c = re.search(r'^((\\S{3})*)((ATG)(\\S{3})*?(TAG|TAA|TGA))', s4[1:])\n",
    "print(f'm4a: {m4a}\\nm4b: {m4b}\\nm4c: {m4c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374afa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "(m4a.groups() if m4a else None, \n",
    "m4b.groups() if m4b else None, \n",
    "m4c.groups() if m4c else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b1a6f",
   "metadata": {},
   "source": [
    "This raises the possibility of using an fstring to modify the search start position instead of slices. With the fstring approach, keep in mind that the curly braces in the regex need to be escaped by doubling them: `{{}}`. Triple braces are needed if we are interpolating a string within escaped braces.\n",
    "\n",
    "Also, fstrings don't allow backslashes in any form. We could replace `\\S` with `.` to accept any character including whitespaces, create a variable to interpolate, or concatenate. \n",
    "\n",
    "Parsing the combination of single, double, and triple braces in this expression turns into bit of a mess. Regex is less concise on net because either way some `current_index` needs to be updated with `+= len(str) + 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '\\S'\n",
    "i = 0\n",
    "m4a = re.search(f'^({s}{{{i}}}({s}{{3}})*)((ATG)({s}{{3}})*?(TAG|TAA|TGA))', s4)\n",
    "i = 1\n",
    "m4b = re.search(f'^({s}{{{i}}}({s}{{3}})*)((ATG)({s}{{3}})*?(TAG|TAA|TGA))', s4)\n",
    "i = 2\n",
    "m4c = re.search(f'^({s}{{{i}}}({s}{{3}})*)((ATG)({s}{{3}})*?(TAG|TAA|TGA))', s4)\n",
    "print(f'm4a: {m4a}\\nm4b: {m4b}\\nm4c: {m4c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Iterable\n",
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "# Not mutable so all desired attributes need to be set at once.\n",
    "# More prelude to turning everything into a class a la BioPython...\n",
    "\n",
    "orf = namedtuple('ORF', \n",
    "                 names:=['sequence', 'start', 'end', 'length', 'readingframe'],\n",
    "                 defaults=[None for _ in range(len(names))])\n",
    "\n",
    "# Helper function to retrieve one ORF from any frame.\n",
    "# def getorf(seq: str, pattern: re.Pattern, startpos=0, endpos: int=...) -> orf:\n",
    "#     if match := pattern.search(seq, \n",
    "#                                startpos, \n",
    "#                                endpos if endpos is not ... else len(seq)):\n",
    "#         sequence = match.groupdict()['orf']\n",
    "#         # 'start' is the index and 'end' is the \"character number\" index + 1, \n",
    "#         # so they can be used directly for slicing.\n",
    "#         start = startpos + len(match.groupdict()['prior'])\n",
    "#         length = len(sequence)\n",
    "#         end = start + length\n",
    "#         readingframe = start % 3\n",
    "#         return orf(sequence, start, end, length, readingframe)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "def getorfs_re(seq: str, startpos: int = 0, endpos: int = ..., \n",
    "        frame: int = ..., seqtype = 'DNA', stopcodons = None) -> list[orf]:\n",
    "    '''Any character is allowed between start and stop codons.\n",
    "    Reads to end of string if 'endpos' is None.\n",
    "    'frame' can be any int < (len(seq) - startpos) and is interpreted relative to 'startpos'. \n",
    "    (startpos=x, frame=y) is equivalent to (startpos=x+y, frame=0) and \n",
    "    (startpos=0, frame=x+y).\n",
    "    Assumes standard stop codons but an alternative set of stop codons may be used.\n",
    "    Results sorted by start position.\n",
    "    Reports start as index and end as index + 1 so they can be used directly\n",
    "    in slices or range().\n",
    "    '''\n",
    "    startpos = startpos if frame is ... else startpos + frame\n",
    "    endpos = len(seq) if endpos is ... else endpos\n",
    "    stopcodons = '|'.join(stopcodons if stopcodons is not None\n",
    "        else ['UAG','UGA','UAA'] if seqtype == 'RNA' or seqtype == 'rna' \n",
    "        else ['TAG','TGA','TAA']) \n",
    "    orfs = []\n",
    "\n",
    "    # triplet searches must be lazy\n",
    "    s = '\\S'\n",
    "    pattern = re.compile(f\"(?P<prior>^{f'{s}*?' if frame is ... else '(.{3})*?'})\"\n",
    "                        +f\"(?P<orf>(ATG)(.{{3}})*?({stopcodons}))\", re.I)\n",
    "    while (nextorf := pattern.search(seq[startpos:endpos])):\n",
    "        sequence = nextorf.groupdict()['orf']\n",
    "        # 'start' is the index and 'end' is the \"character number\" index + 1, \n",
    "        # so they can be used directly for slicing.\n",
    "        start = startpos + len(nextorf.groupdict()['prior'])\n",
    "        length = len(sequence)\n",
    "        end = start + length\n",
    "        readingframe = start % 3 + 1\n",
    "        orfs.append(orf(sequence, start+1, end, length, readingframe))\n",
    "        startpos = start + 3\n",
    "    return orfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd51001",
   "metadata": {},
   "source": [
    "#### That was a nightmare to debug...\n",
    "and I ended up using slices anyway. The easiest thing would be to cheat and find all ORFs, then just filter the ones that are not in frame. \n",
    "\n",
    "Another KISS approach would be to collect the indices of start and stop codons in separate lists, then evaluate whether the distances between stop and start codons are divisible by 3 and return slices containing valid start-stop pairs. \n",
    "\n",
    "While iterating through the list of stop codons for each start codon is exponential behavior, it is way more efficient than a) the regex search method or b) looking for a start codon then looking for a stop codon, then going back to find the next start codon and looking for its matching stop codon, etc.\n",
    "Some further optimization could be done to avoid iterating through stop codons that are known to be before the current start codon, but that is really not necessary for most cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Iterable\n",
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "# Not mutable so all desired attributes need to be set at once.\n",
    "# More prelude to turning everything into a class a la BioPython...\n",
    "\n",
    "orf = namedtuple('ORF', \n",
    "                 names:=['sequence', 'start', 'end', 'length', 'readingframe'],\n",
    "                 defaults=[None for _ in range(len(names))])\n",
    "\n",
    "def getorfs(seq: str, startpos: int = 1, endpos: int = ..., frame: int = ...,\n",
    "            seqtype = 'DNA', stopcodons: Iterable = ...) -> list[orf]:\n",
    "    '''\n",
    "    Arguments and return values are indexed to 1. #don'tblameme\n",
    "    Any character is allowed between start and stop codons.\n",
    "    Search is case-insensitive but return values preserve case.\n",
    "    'frame' is interpreted relative to 'startpos' with an index of 1.\n",
    "    (startpos=x, frame=y) is equivalent to (startpos=y, frame=x).\n",
    "    Reads to end of string if 'endpos' is None.\n",
    "    Assumes standard stop codons but an alternative set of stop codons may be used.\n",
    "    Results sorted by start position.\n",
    "    Reports start as index and end as index + 1 so they can be used directly\n",
    "    in slices or range().\n",
    "    '''\n",
    "    startpos = startpos - 1 if frame is ... else startpos + frame - 2\n",
    "    endpos = len(seq) if endpos is ... else endpos\n",
    "    stopcodons = (stopcodons.upper() if stopcodons is not ... \n",
    "        else ['UAG','UGA','UAA'] if seqtype.upper() == 'RNA'\n",
    "        else ['TAG','TGA','TAA'])\n",
    "    orfs = []\n",
    "    raised = seq.upper()\n",
    "    starts = [i for i in range(startpos, endpos-3) \n",
    "              if raised[i:i+3] == 'ATG' \n",
    "              and (True if frame is ... else (i - startpos) % 3 == 0)]\n",
    "    stops = [i for i in range(len(seq)-2)       # len-2 keeps seq[-3] in range.\n",
    "             if (codon := raised[i:i+3])\n",
    "             and any(codon == stop for stop in stopcodons) \n",
    "             and (True if frame is ... else (i - startpos) % 3 == 0)]\n",
    "    for start in starts:\n",
    "        for stop in stops:\n",
    "            if start < stop and ((stop:= stop+3) - start) % 3 == 0:\n",
    "               orfs.append(orf(seq[start:stop], \n",
    "                               start+1, stop, stop-start, start%3+1))\n",
    "               break\n",
    "    return orfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df490a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((getorfs_re(s4), getorfs_re(s4,frame=1), getorfs_re(s4,frame=7)) == \n",
    "    (getorfs(s4), getorfs(s4,frame=2), getorfs(s4,frame=8)),'\\n')\n",
    "\n",
    "print(getorfs(s4), getorfs(s4,startpos=20), getorfs(s4,frame=7),sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from collections import defaultdict\n",
    "from itertools import takewhile\n",
    "\n",
    "nestedseqdict = fasta_to_nesteddict(path_to_fasta_file)\n",
    "newseqdict = updated_ndict(\n",
    "    nestedseqdict,['Length', 'ORFs'],[len, getorfs], modkey='Sequence')\n",
    "iter1 = iter(nestedseqdict.items())\n",
    "iter2 = iter(newseqdict.items())\n",
    "# pprint([next(iter1) for _ in range(4)])\n",
    "# pprint([next(iter2) for _ in range(4)])\n",
    "orfsbysize = defaultdict(list)\n",
    "for header, seq in newseqdict.items():\n",
    "    seq['ORFs'].sort(key=attrgetter('length'))\n",
    "    shortest = seq['ORFs'][0].length\n",
    "    longest = seq['ORFs'][-1].length\n",
    "    # use iterator to collect all sequences with the min or max length\n",
    "    orfsbysize[shortest].extend([\n",
    "        (header, shortorf) for shortorf in \n",
    "        takewhile(lambda x: x.length == shortest, seq['ORFs'])\n",
    "        ])\n",
    "    orfsbysize[longest].extend([\n",
    "        (header, longorf) for longorf in \n",
    "        takewhile(lambda x: x.length == longest, seq['ORFs'][::-1])\n",
    "        ])    \n",
    "\n",
    "orfsbysize = sorted(orfsbysize.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47699890",
   "metadata": {},
   "source": [
    "### Looking for repeats of size N\n",
    "\n",
    "Tempting to try regex again like `(.{N}){2,}`. We've seen before that regex can be unwieldy and at times difficult to predict, but it's so powerful once the magic expression is found. Here we want to be able to detect overlapping, consecutive, and non-consecutive repeats, and count their frequency. `str.count()` does not count overlapping occurrences. It would also be nice to save the locations of the repeats but first things first: identifying repeated sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d085c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = 'agacagacagacagt'\n",
    "s6 = 'agacagacjhgfcjhfgacagacagt'\n",
    "s7 = 'agacagacjhgfcjhfgacagatcjh'\n",
    "n = 3\n",
    "s = '\\S'\n",
    "# `?=` means lookahead without consuming the string so we can find overlapping matches\n",
    "# matches any group of n non-whitespace characters \n",
    "expr = re.compile(f'(?=(?P<repeat>{s}{{{n}}}))')\n",
    "# Matches groups of n non-whitespace characters that are later repeated. \n",
    "# Need to use +? so next repeat starts at least one char from beginning of first repeat.\n",
    "# Resulting list contains duplicates of any group that contains >1 repeat. \n",
    "# Any pair of repeats is a match since the string is not consumed by lookahead.\n",
    "expr = re.compile(f'(?=(?P<repeat>{s}{{{n}}}))(?={s}+?(?P=repeat))')\n",
    "# Changing to greedy evaluation of spacers does not change this, \n",
    "# nor does matching repeats of the entire pattern, because these do not change\n",
    "# the validity of any other combination of repeats. \n",
    "# expr = re.compile(f'((?=(?P<repeat>{s}{{{n}}}))(?={s}+(?P=repeat)))+')\n",
    "# expr.findall(s5)\n",
    "# expr.findall(s6)\n",
    "expr.findall(s7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e186ea",
   "metadata": {},
   "source": [
    "### Named tuple, class, and dataclass.\n",
    "\n",
    "- Sorting and attribute access are faster with named tuples, but named tuples cannot have methods like classes. Methods have to be defined as normal functions.\n",
    "  - Named tuples are immutable, but they can contain mutable objects since the tuple just stores references to objects. The reference to a mutable object like a list does not change when it is mutated.\n",
    "<br><br>\n",
    "- Methods can be defined within classes which helps keep code organized logically. It also allows having methods for different classes to have the same name but different functionality or implementation, and makes it easier to track how a function is intended to be used.\n",
    "  - Classes are not only mutable, but class instances can also be assigned attributes that were not defined in the class definition. This is because by default, attributes are stored in a dictionary. If dataclasses are \"mutable named tuples with defaults\", classes are \"named lists with special methods\".\n",
    "<br><br>\n",
    "- Despite the \"mutable named tuples with defaults\" nickname, dataclasses are more like customizable pre-built classes. They benefit from inheritance and can have their own methods. The most obvious difference is how convenient they are to define, similar to named tuples. With the `@dataclass` decorator, one does not even have to define `__init__`, much less `__str__` or `__repr__`, along with a whole host of other methods like comparisons and hashing. \n",
    "  - Supplying mutable default values is performed using a default factory function, Ã  la `field(default_factory=list)` rather than the `x = [] if x is None else x` idiom.\n",
    "  - Dataclasses have other associated helper functions like `.astuple()`, `.asdict()`, and `.replace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeattuple = namedtuple('Repeat Tuple', \n",
    "                 names:=['sequence', 'length', 'starts', 'number'],\n",
    "                 defaults=(None,)*len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983604d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e871b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repeat():\n",
    "    def __init__(self, seq, start=None):\n",
    "        self.seq = seq\n",
    "        self.len = len(seq)\n",
    "        self.start = start\n",
    "    def __repr__(self) -> str:\n",
    "        repstr = '(' + ', '.join(f'{attr}={repr(val)}'\n",
    "                                for attr, val in self.__dict__.items()) + ')'\n",
    "        return self.__class__.__name__ + repstr\n",
    "    def __str__(self) -> str:\n",
    "        return self.seq\n",
    "\n",
    "    def where(self, seq, inframe=False):\n",
    "        'Returns start positions indexed to 1.'\n",
    "        starts, pos = [], 0\n",
    "        while (pos := seq.find(self.seq, pos)) != -1:\n",
    "            pos += 1\n",
    "            if (pos % 3 == 1 if inframe else True):\n",
    "                starts.append(pos)\n",
    "        return starts\n",
    "    def count(self, seq, inframe=False):\n",
    "        'Use if locations are not needed. Otherwise len(starts) from where()can be used.'\n",
    "        count, pos = 0, 0\n",
    "        while (pos := seq.find(self.seq, pos)) != -1:\n",
    "            pos += 1\n",
    "            if (pos % 3 == 1 if inframe else True):\n",
    "                count += 1\n",
    "        return count\n",
    "s6 = 'agacagacjhgfcjhfgacagacagt'\n",
    "r = Repeat('acaga')\n",
    "r.count(s6, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e871b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repeat():\n",
    "    def __init__(self, seq, start=None):\n",
    "        self.seq = seq\n",
    "        self.len = len(seq)\n",
    "        self.start = start\n",
    "    def __repr__(self) -> str:\n",
    "        repstr = '(' + ', '.join(f'{attr}={repr(val)}'\n",
    "                                for attr, val in self.__dict__.items()) + ')'\n",
    "        return self.__class__.__name__ + repstr\n",
    "    def __str__(self) -> str:\n",
    "        return self.seq\n",
    "\n",
    "    def where(self, seq, inframe=False):\n",
    "        'Returns start positions indexed to 1.'\n",
    "        starts, pos = [], 0\n",
    "        while (pos := seq.find(self.seq, pos)) != -1:\n",
    "            pos += 1\n",
    "            if (pos % 3 == 1 if inframe else True):\n",
    "                starts.append(pos)\n",
    "        return starts\n",
    "    def count(self, seq, inframe=False):\n",
    "        'Use if locations are not needed. Otherwise len(starts) from where()can be used.'\n",
    "        count, pos = 0, 0\n",
    "        while (pos := seq.find(self.seq, pos)) != -1:\n",
    "            pos += 1\n",
    "            if (pos % 3 == 1 if inframe else True):\n",
    "                count += 1\n",
    "        return count\n",
    "s6 = 'agacagacjhgfcjhfgacagacagt'\n",
    "r = Repeat('acaga')\n",
    "r.count(s6, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.newthing = 9\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc95e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findrepeats(seq, length, inframe=False) -> list[Repeat]:\n",
    "    \"\"\"\n",
    "    Subequences that have overlapping, consecutive, or non-consecutive repeats.\n",
    "    Sequences with no in-frame repeats can be filtered out, \n",
    "    but any sequences that have at least two in frame copies \n",
    "    (i.e. are repeated at least once in frame) will always be included,\n",
    "    even if the sequence is also repeated out of frame.\"\"\"\n",
    "    s = '\\S'                            # Match any non-whitespace characters.\n",
    "    expr = re.compile(f'(?=(?P<repeat>{s}{{{length}}}))(?={s}+?(?P=repeat))')\n",
    "    repeated = set(expr.findall(seq))\n",
    "    return [Repeat(repeat) for repeat in repeated]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(['aga', 'gac', 'aca', 'cag', 'aga', 'gac', 'cjh', 'cjh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted([orf(sequence='ATGCGAATGGCTGATGCGGTAGTAATCTGA', start=6, end=36, length=30, readingframe=0), orf(sequence='ATGGCTGATGCGGTAGTAATCTGA', start=12, end=36, length=24, readingframe=0), orf(sequence='ATGCGGTAG', start=19, end=28, length=9, readingframe=1)], key=attrgetter('sequence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c215212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1496728fd4910581be769dbea2647f5ee73a9be9d1697e51a4ff491ad13ee4c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('working': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
